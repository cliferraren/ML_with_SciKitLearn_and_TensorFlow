![png](images/ml_image.png)
---
Here, we learned Machine learning which is the science of getting computers to act without being explicitly programmed.



**[Data Preprocessing:](https://github.com/cliferraren/Machine-Learning/tree/master/Data%20PreProcessing)**

 1. [Data Preprocessing - Brain Categorical](https://github.com/cliferraren/Machine-Learning/blob/master/Data%20PreProcessing/Data%20Preprocessing.ipynb)
 2. [Data Preprocessing - Respiratory Disease](https://github.com/cliferraren/Machine-Learning/blob/master/Data%20PreProcessing/Respiratory%20Disease.ipynb)
 
 
---


**[Supervised Learning:](https://github.com/cliferraren/Machine-Learning/tree/master/Supervised)**


A. [Regression:](https://github.com/cliferraren/Machine-Learning/tree/master/Supervised/Linear%20Regression)
    > Regression is the task of predicting a continuous quantity.
    
    
 1. [Univariate Linear Regression - Intro](https://github.com/cliferraren/Machine-Learning/blob/master/Supervised/Linear%20Regression/Univariate_Linear_Regression.ipynb) 
    > Linear Regression is a fundamental algorithm in machine learning. It is used as a building block for other ML models. LR is fast and easy to understand, calculate, and interpret. Often good enough. Don't over-engineer your solution. If your data is linear then use a linear model.

    Demo:  [Linear Regression - LSD DATA](https://github.com/cliferraren/Machine-Learning/blob/master/Supervised/Linear%20Regression/LinearRegression_LSD_DATA.ipynb)
    
 2. [Quantifying Regression - Intro](https://github.com/cliferraren/Machine-Learning/blob/master/Supervised/Linear%20Regression/Quantifying_Regression.ipynb)
    Demo:  [Quantifying Regression - Brain Data](https://github.com/cliferraren/Machine-Learning/blob/master/Supervised/Linear%20Regression/Quantifying%20Linear%20Regression_Brain.ipynb)
    
 3. [Multi Linear Regression - Intro](https://github.com/cliferraren/Machine-Learning/blob/master/Supervised/Linear%20Regression/MultiVariate%20Linear%20Regression.ipynb)
    Demo:  [Multi Linear Regression - Beer Foam Data](https://github.com/cliferraren/Machine-Learning/blob/master/Supervised/Linear%20Regression/MultiLinear%20Regression%20-%20Beer.ipynb)
    
    Demo: [Linear Regression - Life Satisfaction](https://github.com/cliferraren/Machine-Learning/blob/master/Supervised/Linear%20Regression/LifeSatisfaction_vs_Income.ipynb)

---


B. [Classification:](https://github.com/cliferraren/Machine-Learning/tree/master/Supervised/Logistic%20Regression)
    > Classification is the task of predicting a discrete class label.
    
    
1. [Logistic Regression - Intro](https://github.com/cliferraren/Machine-Learning/blob/master/Supervised/Logistic%20Regression/Logistic%20Regression%20-Intro.ipynb)
    > Logistic regression is the appropriate regression analysis to conduct when the dependent variable is dichotomous (binary). Like all regression analyses, the logistic regression is a predictive analysis.
    
    Demo: [Logistic Regression - Voice Gender Recognition](https://github.com/cliferraren/Machine-Learning/blob/master/Supervised/Logistic%20Regression/Voice%20Recognition.ipynb)
    

2. [Decision Trees - Intro ](https://github.com/cliferraren/Machine-Learning/blob/master/Supervised/Decision%20Trees%20%26%20Random%20Forests/Decision%20Trees.ipynb)
    > Prefers problems with categorical data
    > Becomes less useful on problems with low covariance
    
3. [Random Forests](https://github.com/cliferraren/Machine-Learning/blob/master/Supervised/Decision%20Trees%20%26%20Random%20Forests/Random%20Forests.ipynb)
    > Instead of a single, complex tree like in the previous slide, a random forest algorithm will sample the data and build many smaller, simpler decisions trees (i.e. A forest of trees). Each of these trees are much simpler because they are built from a subset of the data. Each simple tree is considered a “weak classifier”, but when you combine them, they form a “strong classifier”
    
    Demo:  [Diabetes Data using Decision Trees Classifier](https://github.com/cliferraren/Machine-Learning/blob/master/Supervised/Decision%20Trees%20%26%20Random%20Forests/DecisionTree%20Classifier-%20Diabetes%20Data.ipynb)
    
5. [K-Nearest Neighbors]
    > KNN is good for measuring distance-based approximations; it suffers from the curse of dimensionality