![png](images/ml_image.png)
---
Here, we learned Machine learning which is the science of getting computers to act without being explicitly programmed.



**[Data Preprocessing:](https://github.com/cliferraren/Machine-Learning/tree/master/Data%20PreProcessing)**

 1. [Brain Categorical](https://github.com/cliferraren/Machine-Learning/blob/master/Data%20PreProcessing/Data%20Preprocessing.ipynb)
 2. [Respiratory Disease](https://github.com/cliferraren/Machine-Learning/blob/master/Data%20PreProcessing/Respiratory%20Disease.ipynb)
 
 
---

**[Hyperparameter Optimization](https://github.com/cliferraren/Machine-Learning/tree/master/Hyperparameter%20Optimization)**

A. [Grid Search](https://github.com/cliferraren/Machine-Learning/blob/master/Hyperparameter%20Optimization/GridSearch%20-%20Intro.ipynb)

    > Is a technique through which we can come to know the best parameters for a machine learning model.
    > The traditional way of performing hyperparameter optimization has been grid search
    

---

**[Supervised Learning:](https://github.com/cliferraren/Machine-Learning/tree/master/Supervised)**


A. [Regression:](https://github.com/cliferraren/Machine-Learning/tree/master/Supervised/Regression)
    > is the task of predicting a continuous quantity.
    
    
 1. [Univariate Linear Regression - Intro](https://github.com/cliferraren/Machine-Learning/blob/master/Supervised/Regression/Univariate_Linear_Regression.ipynb) 
    > Linear Regression is a fundamental algorithm in machine learning. It is used as a building block for other ML models. LR is fast and easy to understand, calculate, and interpret. Often good enough. Don't over-engineer your solution. If your data is linear then use a linear model.

    Demo:  [Linear Regression - LSD DATA](https://github.com/cliferraren/Machine-Learning/blob/master/Supervised/Regression/LinearRegression_LSD_DATA.ipynb)
    
 2. [Quantifying Regression - Intro](https://github.com/cliferraren/Machine-Learning/blob/master/Supervised/Regression/Quantifying_Regression.ipynb)
    
    Demo:  [Quantifying Regression - Brain Data](https://github.com/cliferraren/Machine-Learning/blob/master/Supervised/Regression/Quantifying%20Linear%20Regression_Brain.ipynb)
    
 3. [Multi Linear Regression - Intro](https://github.com/cliferraren/Machine-Learning/blob/master/Supervised/Regression/MultiVariate%20Linear%20Regression.ipynb)
    
    Demo:  [Multi Linear Regression - Beer Foam Data](https://github.com/cliferraren/Machine-Learning/blob/master/Supervised/Regression/MultiLinear%20Regression%20-%20Beer.ipynb)
    
    Demo: [Linear Regression - Life Satisfaction](https://github.com/cliferraren/Machine-Learning/blob/master/Supervised/Regression/LifeSatisfaction_vs_Income.ipynb)

---


B. [Classification:](https://github.com/cliferraren/Machine-Learning/tree/master/Supervised/Classification)
    > is the task of predicting a discrete class label.
    
    
1. [Logistic Regression - Intro](https://github.com/cliferraren/Machine-Learning/blob/master/Supervised/Classification/Logistic%20Regression/Logistic%20Regression%20-Intro.ipynb)
    > Logistic regression is the appropriate regression analysis to conduct when the dependent variable is dichotomous (binary). Like all regression analyses, the logistic regression is a predictive analysis.
    
    Demo: [Logistic Regression - Voice Gender Recognition](https://github.com/cliferraren/Machine-Learning/blob/master/Supervised/Classification/Logistic%20Regression/Voice%20Recognition.ipynb)
    

2. [Decision Trees - Intro ](https://github.com/cliferraren/Machine-Learning/blob/master/Supervised/Classification/Decision%20Trees%20%26%20Random%20Forests/Decision%20Trees.ipynb)
    > Prefers problems with categorical data.
    
    > Becomes less useful on problems with low covariance
    
3. [Random Forests](https://github.com/cliferraren/Machine-Learning/blob/master/Supervised/Classification/Decision%20Trees%20%26%20Random%20Forests/Random%20Forests.ipynb)
    > Instead of a single, complex tree like in the previous slide, a random forest algorithm will sample the data and build many smaller, simpler decisions trees (i.e. A forest of trees). Each of these trees are much simpler because they are built from a subset of the data. Each simple tree is considered a “weak classifier”, but when you combine them, they form a “strong classifier”
    
    Demo:  [Decision Trees - Diabetes Data](https://github.com/cliferraren/Machine-Learning/blob/master/Supervised/Classification/Decision%20Trees%20%26%20Random%20Forests/DecisionTree%20Classifier-%20Diabetes%20Data.ipynb)
    
5. [K-Nearest Neighbors](https://github.com/cliferraren/Machine-Learning/blob/master/Supervised/Classification/K_Nearest_Neighbors/K_Nearest_Neighbors%20-%20Intro.ipynb)
    > KNN is good for measuring distance-based approximations; it suffers from the curse of dimensionality.

    Demo: [K-Nearest Neighbors - Diabetes Data](https://github.com/cliferraren/Machine-Learning/blob/master/Supervised/Classification/K_Nearest_Neighbors/K_Nearest_Neighbors%20-%20Diabetes.ipynb)

6. [Support Vector Machine](https://github.com/cliferraren/Machine-Learning/blob/master/Supervised/Classification/SVM/Support%20Vector%20Machine%20-%20Intro.ipynb)
    > Works where there is a definite distinction between two classifications.
    
    > Prefers binary classification problems.
    
    Demo: [Support Vector Machine - Diabetes Data](https://github.com/cliferraren/Machine-Learning/blob/master/Supervised/Classification/SVM/Support%20Vector%20Machine%20-%20Diabetes.ipynb)

